HYPERPARAMETERS AFTER CHANGES
===============================
Date: 5 f√©vrier 2026

Training Configuration:
- Number of epochs: 80
- Learning rate: 0.05 (changed from 0.1)
- Batch size: 64 (changed from 32)
- Optimizer: AdamW (changed from SGD)
  - Weight decay: 1e-4 (changed from 5e-4)

Learning Rate Scheduler:
- Type: StepLR (changed from CosineAnnealingLR)
- Step size: 30
- Gamma: 0.1

Model:
- Architecture: ResNet18

Dataset:
- Training samples: 15,000 (subset)
- Test samples: 10,000
- Data augmentation: RandomCrop(32, padding=4), RandomHorizontalFlip

Loss Function:
- CrossEntropyLoss

Device:
- GPU (if available) / CPU

Expected Impact:
- Lower learning rate (0.05) should provide more stable convergence
- AdamW optimizer with adaptive learning rates may converge faster
- Larger batch size (64) provides more stable gradients
- StepLR will reduce learning rate by 10x every 30 epochs
- Lower weight decay (1e-4) may reduce overfitting slightly
