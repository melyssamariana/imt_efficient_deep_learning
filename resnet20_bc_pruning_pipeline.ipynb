{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet20 1W1A (BinaryConnect) + Pruning\n",
    "\n",
    "Notebook para executar seu fluxo completo com:\n",
    "- ResNet20 binarizada (1W1A) via `BinaryConnect`\n",
    "- Pruning estruturado, nao estruturado ou combinado\n",
    "- Treino, avaliacao e salvamento de checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute esta celula uma vez para instalar dependencias no kernel atual.\n",
    "%pip install -U pip\n",
    "%pip install numpy matplotlib tqdm wandb\n",
    "\n",
    "# PyTorch CPU (padrao)\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "# Se voce for usar GPU NVIDIA (CUDA 12.1), comente a linha CPU acima e use esta:\n",
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from cifar10 import trainloader, trainloader_subset, testloader\n",
    "from models.resnet_s import resnet20\n",
    "from models.binaryconnect import BC, BinaryActivationSTE\n",
    "from techniques.quantization import QuantizationAwareConfig\n",
    "from techniques.prunning import PrunningStructured, PrunningUnstructured\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "CFG = {\n",
    "    \"label\": \"rn20a1_notebook\",\n",
    "    \"project_name\": \"imt_efficient_dl\",\n",
    "    \"path_backup\": \"./\",\n",
    "    \"checkpoint_path\": \"./resnet20_bc1w1a_best.pth\",\n",
    "    \"use_checkpoint\": True,\n",
    "    \"use_subset\": False,\n",
    "    \"use_wandb\": False,\n",
    "    \"run_training\": False,\n",
    "    \"num_epochs\": 50,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"weight_decay\": 1e-3,\n",
    "    \"momentum\": 0.9,\n",
    "    \"nesterov\": True,\n",
    "    \"label_smoothing\": 0.1,\n",
    "    \"warmup_epochs\": 5,\n",
    "    \"early_stopping_patience\": 150,\n",
    "    \"early_stopping_min_delta\": 0.0,\n",
    "    \"pruning_method\": \"combined\",\n",
    "    \"structured_ratios\": [0.05],\n",
    "    \"unstructured_ratios\": [0.7],\n",
    "    \"avoid_overlap\": True,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = trainloader_subset if CFG[\"use_subset\"] else trainloader\n",
    "print(f\"Train samples: {len(train_data.dataset)}\")\n",
    "print(f\"Test samples: {len(testloader.dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _strip_module_prefix(state_dict):\n",
    "    keys = list(state_dict.keys())\n",
    "    if keys and all(k.startswith(\"module.\") for k in keys):\n",
    "        return {k[len(\"module.\"):]: v for k, v in state_dict.items()}\n",
    "    return state_dict\n",
    "\n",
    "def load_bc_checkpoint(model, checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    if isinstance(checkpoint, dict) and \"state_dict\" in checkpoint:\n",
    "        checkpoint = checkpoint[\"state_dict\"]\n",
    "\n",
    "    checkpoint = _strip_module_prefix(checkpoint)\n",
    "\n",
    "    try:\n",
    "        missing, unexpected = model.load_state_dict(checkpoint, strict=False)\n",
    "    except RuntimeError:\n",
    "        checkpoint = {f\"model.{k}\": v for k, v in checkpoint.items()}\n",
    "        missing, unexpected = model.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "    print(f\"Checkpoint carregado: {checkpoint_path}\")\n",
    "    print(f\"Missing keys: {len(missing)} | Unexpected keys: {len(unexpected)}\")\n",
    "\n",
    "\n",
    "base_model = resnet20()\n",
    "num_relu_modules = sum(1 for m in base_model.modules() if isinstance(m, nn.ReLU))\n",
    "if num_relu_modules == 0:\n",
    "    raise RuntimeError(\"resnet20 sem nn.ReLU modulos. A troca para BinaryActivationSTE nao sera aplicada.\")\n",
    "print(f\"nn.ReLU encontrados na ResNet20 base: {num_relu_modules}\")\n",
    "\n",
    "model = BC(base_model).to(device)\n",
    "num_binary_act = sum(1 for m in model.modules() if isinstance(m, BinaryActivationSTE))\n",
    "print(f\"BinaryActivationSTE apos wrapping BC: {num_binary_act}\")\n",
    "\n",
    "if CFG[\"use_checkpoint\"] and os.path.exists(CFG[\"checkpoint_path\"]):\n",
    "    load_bc_checkpoint(model, CFG[\"checkpoint_path\"])\n",
    "else:\n",
    "    print(\"Treinando do zero (checkpoint nao carregado).\")\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = QuantizationAwareConfig(\n",
    "    label=CFG[\"label\"],\n",
    "    model=model,\n",
    "    train_data=train_data,\n",
    "    test_data=testloader,\n",
    "    project_name=CFG[\"project_name\"],\n",
    "    path_backup=CFG[\"path_backup\"],\n",
    "    wand_on=CFG[\"use_wandb\"],\n",
    "    input_dtype=\"bc\",\n",
    "    num_epochs=CFG[\"num_epochs\"],\n",
    "    learning_rate=CFG[\"learning_rate\"],\n",
    "    weight_decay=CFG[\"weight_decay\"],\n",
    "    optimizer_name=\"SGD\",\n",
    "    momentum=CFG[\"momentum\"],\n",
    "    nesterov=CFG[\"nesterov\"],\n",
    "    label_smoothing=CFG[\"label_smoothing\"],\n",
    "    warmup_epochs=CFG[\"warmup_epochs\"],\n",
    "    scheduler_name=\"LinearLR+CosineAnnealingLR\",\n",
    "    early_stopping_patience=CFG[\"early_stopping_patience\"],\n",
    "    early_stopping_min_delta=CFG[\"early_stopping_min_delta\"],\n",
    "    pruning_method=CFG[\"pruning_method\"],\n",
    "    structured_ratios=CFG[\"structured_ratios\"],\n",
    "    unstructured_ratios=CFG[\"unstructured_ratios\"],\n",
    "    avoid_overlap=CFG[\"avoid_overlap\"],\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "cfg.criterion = nn.CrossEntropyLoss(label_smoothing=cfg.label_smoothing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_opt_and_sched(cfg_obj):\n",
    "    cfg_obj.optimizer = optim.SGD(\n",
    "        cfg_obj.model.parameters(),\n",
    "        lr=cfg_obj.learning_rate,\n",
    "        momentum=cfg_obj.momentum,\n",
    "        weight_decay=cfg_obj.weight_decay,\n",
    "        nesterov=cfg_obj.nesterov,\n",
    "    )\n",
    "\n",
    "    warmup = optim.lr_scheduler.LinearLR(\n",
    "        cfg_obj.optimizer,\n",
    "        start_factor=0.1,\n",
    "        end_factor=1.0,\n",
    "        total_iters=cfg_obj.warmup_epochs,\n",
    "    )\n",
    "    cosine = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        cfg_obj.optimizer,\n",
    "        T_max=max(1, cfg_obj.num_epochs - cfg_obj.warmup_epochs),\n",
    "    )\n",
    "    cfg_obj.scheduler = optim.lr_scheduler.SequentialLR(\n",
    "        cfg_obj.optimizer,\n",
    "        schedulers=[warmup, cosine],\n",
    "        milestones=[cfg_obj.warmup_epochs],\n",
    "    )\n",
    "\n",
    "\n",
    "reset_opt_and_sched(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_prune = [\n",
    "    (module, \"weight\")\n",
    "    for module in cfg.model.modules()\n",
    "    if isinstance(module, (nn.Conv2d, nn.Linear))\n",
    "]\n",
    "\n",
    "prune_unstructured = PrunningUnstructured(\n",
    "    cfgModel=cfg,\n",
    "    ratios=cfg.unstructured_ratios,\n",
    "    params_to_prune=params_to_prune,\n",
    ")\n",
    "\n",
    "prune_structured = PrunningStructured(\n",
    "    cfgModel=cfg,\n",
    "    ratios=cfg.structured_ratios,\n",
    "    params_to_prune=params_to_prune,\n",
    ")\n",
    "\n",
    "print(f\"Camadas prunaveis: {len(params_to_prune)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.pruning_method == \"unstructured\":\n",
    "    pruned_model = prune_unstructured.unstructured(\n",
    "        ratios=cfg.unstructured_ratios,\n",
    "    )\n",
    "elif cfg.pruning_method == \"structured\":\n",
    "    pruned_model = prune_structured.structured(\n",
    "        ratios=cfg.structured_ratios,\n",
    "    )\n",
    "elif cfg.pruning_method == \"combined\":\n",
    "    pruned_model = prune_structured.combined(\n",
    "        structured_ratios=cfg.structured_ratios,\n",
    "        unstructured_ratios=cfg.unstructured_ratios,\n",
    "        avoid_overlap=cfg.avoid_overlap,\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"pruning_method invalido: {cfg.pruning_method}. Use unstructured, structured ou combined.\"\n",
    "    )\n",
    "\n",
    "if pruned_model is not None:\n",
    "    cfg.model = pruned_model\n",
    "    reset_opt_and_sched(cfg)\n",
    "\n",
    "print(\"Pruning aplicado com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cfg.calculate_score(\n",
    "    p_s=cfg.structured_ratios[0],\n",
    "    p_u=cfg.unstructured_ratios[0],\n",
    "    q_w=1,\n",
    "    q_a=1,\n",
    "    w=sum(p.numel() for p in cfg.model.parameters()),\n",
    "    f=40.55e6,\n",
    ")\n",
    "print(f\"Score estimado: {score:.6f}\")\n",
    "\n",
    "loss_before, acc_before = cfg.evaluate()\n",
    "print(f\"Antes do treino -> Loss: {loss_before:.4f} | Acc: {acc_before:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG[\"run_training\"]:\n",
    "    cfg.train_loop()\n",
    "else:\n",
    "    print(\"Treino desabilitado. Ajuste CFG['run_training'] = True para treinar.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_after, acc_after = cfg.evaluate()\n",
    "print(f\"Estado final -> Loss: {loss_after:.4f} | Acc: {acc_after:.2f}%\")\n",
    "\n",
    "pruned_path = os.path.join(\n",
    "    CFG[\"path_backup\"],\n",
    "    f\"pruned_model_{cfg.pruning_method}_{cfg.label}.pth\",\n",
    ")\n",
    "torch.save(cfg.model.state_dict(), pruned_path)\n",
    "print(f\"Modelo salvo em: {pruned_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso rapido\n",
    "\n",
    "1. Ajuste o dicionario `CFG`.\n",
    "2. Execute todas as celulas em ordem.\n",
    "3. Para treinar de fato, use `CFG['run_training'] = True`.\n",
    "4. Para alternar pruning: `unstructured`, `structured` ou `combined`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
